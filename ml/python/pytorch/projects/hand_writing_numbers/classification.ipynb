{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Mnist分类任务：\n",
    "\n",
    "- 网络基本构建与训练方法，常用函数解析\n",
    "- torch.nn.functional模块\n",
    "- nn.Module模块"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取 mnist数据集\n",
    "\n",
    "- 会自动下载数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "import gzip\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "DATA_PATH = Path('../../data')\n",
    "FILE_PATH = DATA_PATH / 'mnist' # 类型是 WindowsPath('../data/mnist')\n",
    "FILE_PATH.mkdir(parents=True,exist_ok=True)\n",
    "# URL = 'http://deeplearning.net/data/mnist/'\n",
    "URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/'\n",
    "FILE_NAME = 'mnist.pkl.gz'\n",
    "\n",
    "# 如果不存在就下载\n",
    "if not (FILE_PATH / FILE_NAME).exists():\n",
    "    content = requests.get(URL+FILE_NAME).content\n",
    "    (FILE_PATH/FILE_NAME).open(\"wb\").write(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# with 就是自动关闭流的操作. 跟try(){一个概念, 然后后面根的就是流的名称\n",
    "# as_posix()作用于文件路径上 意思就是将文件路径转换为一个\"/\"路径,用于windows, 比如 'c:/application/a/b/c'\n",
    "# pickle工具是序列化工具可以将文件读取成为数据, 也可以写入硬盘 pickle.load就是将文件序列化读出来\n",
    "with gzip.open((FILE_PATH/FILE_NAME).as_posix(),'rb') as f:\n",
    "    ((x_train,y_train),(x_valid,y_valid),_)=pickle.load(f,encoding='latin-1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "每一个文件都是28*28=784个像素点的文件. 每一个样本都是一样的"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(50000, 784)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[0].reshape((28,28)),cmap=\"gray\")\n",
    "print(x_train.shape) # 这里我们用了灰度颜色通道. 并没有使用彩色颜色通道, 因此就是28*28*1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们使用onehotencoding编码, 最终得到10个概率. 也就是最后的输出一层为10各节点来返回每一个数字的可能性概率. 因此在训练的时候需要将ylabels进行onehot处理\n",
    "\n",
    "![image](./assets/4.png)\n",
    "\n",
    "![image](./assets/5.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# map方法, 第一个参数是处理方法\n",
    "# 第二个参数是目标数据, 输出的结果也是解构的结果.\n",
    "x_train,y_train,x_valid,y_valid=map(torch.tensor,(x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.nn.functional 很多层和函数在这里都会见到\n",
    "torch.nn.functional中有很多功能，后续会常用的。\n",
    "\n",
    "那什么时候使用nn.Module，什么时候使用nn.functional呢？\n",
    "\n",
    "一般情况下，如果模型有可学习的参数，也就是有$w_1,w_2...$和$b$的时候, 最好用nn.Module，其他情况nn.functional相对更简单一些"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 损失函数, 这里用的是函数本身而不是返回值. 一般是不带参数的损失函数\n",
    "loss_function = F.cross_entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "bs=64\n",
    "xb=x_train[0:bs] # 切片操作, 将x_train看做一个array. 然后获取array的0到64个元素\n",
    "yb=y_train[0:bs]\n",
    "\n",
    "# 随机一些权重\n",
    "weights = torch.randn([784,10],dtype=torch.float,requires_grad=True)\n",
    "bias = torch.zeros(10,requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor(12.1908, grad_fn=<NllLossBackward0>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def model(data):\n",
    "    return data.mm(weights) + bias\n",
    "\n",
    "print(loss_function(model(xb),yb)) # 直接传入预测值和标签值就可以了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创建一个model来更简化代码\n",
    "- 必须继承nn.Module且在其构造函数中需调用nn.Module的构造函数\n",
    "- 无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播\n",
    "- Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# 集成nn.Module\n",
    "class Minist_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 必须集成构造函数, 要求的\n",
    "        super().__init__()\n",
    "        self.hidden1=nn.Linear(784,128) # 创建隐层\n",
    "        self.hidden2=nn.Linear(128,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        # 定义前向传播. 对隐层进行\n",
    "        x=F.relu(self.hidden1(x))\n",
    "        x=F.relu(self.hidden2(x))\n",
    "        x=self.out(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Minist_NN(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net = Minist_NN()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以打印我们定义好名字里的权重和偏置项"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "hidden1.weight Parameter containing:\n",
      "tensor([[-0.0265, -0.0195, -0.0033,  ..., -0.0017,  0.0251,  0.0306],\n",
      "        [ 0.0018, -0.0230,  0.0339,  ...,  0.0002, -0.0302,  0.0286],\n",
      "        [-0.0346,  0.0007, -0.0188,  ...,  0.0220,  0.0017,  0.0130],\n",
      "        ...,\n",
      "        [-0.0300, -0.0095,  0.0346,  ..., -0.0214,  0.0023,  0.0181],\n",
      "        [ 0.0345, -0.0091, -0.0332,  ...,  0.0052, -0.0267, -0.0099],\n",
      "        [-0.0139, -0.0030,  0.0022,  ...,  0.0300, -0.0019, -0.0125]],\n",
      "       requires_grad=True) torch.Size([128, 784])\n",
      "hidden1.bias Parameter containing:\n",
      "tensor([-0.0132, -0.0202,  0.0097, -0.0182,  0.0323, -0.0012,  0.0209,  0.0125,\n",
      "         0.0327,  0.0130, -0.0239,  0.0205, -0.0121, -0.0102, -0.0063, -0.0323,\n",
      "         0.0002, -0.0238,  0.0165, -0.0343, -0.0276,  0.0059, -0.0319, -0.0343,\n",
      "         0.0073, -0.0345,  0.0179, -0.0082, -0.0061, -0.0264,  0.0047, -0.0323,\n",
      "        -0.0248, -0.0242, -0.0228,  0.0155, -0.0096, -0.0323, -0.0012,  0.0274,\n",
      "         0.0065,  0.0095, -0.0265, -0.0199, -0.0141, -0.0008, -0.0016,  0.0268,\n",
      "        -0.0167, -0.0222,  0.0220,  0.0130, -0.0058, -0.0264,  0.0131, -0.0146,\n",
      "         0.0335, -0.0073,  0.0283,  0.0133, -0.0159,  0.0299, -0.0148, -0.0011,\n",
      "         0.0106, -0.0263,  0.0139, -0.0256, -0.0096,  0.0095,  0.0102, -0.0051,\n",
      "         0.0039, -0.0339,  0.0097,  0.0259,  0.0301,  0.0222,  0.0007, -0.0054,\n",
      "         0.0201, -0.0356,  0.0272,  0.0090,  0.0198,  0.0210,  0.0137, -0.0305,\n",
      "         0.0038, -0.0147, -0.0331,  0.0035, -0.0273, -0.0329, -0.0248, -0.0064,\n",
      "         0.0295, -0.0254, -0.0201,  0.0039,  0.0210,  0.0307,  0.0041, -0.0333,\n",
      "        -0.0112,  0.0028, -0.0239,  0.0007, -0.0225, -0.0248, -0.0040, -0.0313,\n",
      "         0.0181, -0.0207,  0.0306,  0.0307,  0.0052,  0.0046,  0.0314, -0.0163,\n",
      "         0.0355,  0.0272, -0.0063,  0.0277,  0.0250, -0.0245,  0.0353,  0.0255],\n",
      "       requires_grad=True) torch.Size([128])\n",
      "hidden2.weight Parameter containing:\n",
      "tensor([[-0.0810,  0.0295,  0.0303,  ..., -0.0214, -0.0687,  0.0357],\n",
      "        [ 0.0528,  0.0160, -0.0775,  ..., -0.0877,  0.0168, -0.0369],\n",
      "        [ 0.0013, -0.0275, -0.0384,  ...,  0.0219, -0.0373,  0.0630],\n",
      "        ...,\n",
      "        [ 0.0465,  0.0201, -0.0190,  ..., -0.0181, -0.0347, -0.0034],\n",
      "        [ 0.0837,  0.0790,  0.0425,  ..., -0.0587, -0.0105,  0.0752],\n",
      "        [ 0.0083,  0.0409,  0.0356,  ...,  0.0757, -0.0590,  0.0170]],\n",
      "       requires_grad=True) torch.Size([256, 128])\n",
      "hidden2.bias Parameter containing:\n",
      "tensor([-0.0625, -0.0529, -0.0120, -0.0846, -0.0551,  0.0798, -0.0819, -0.0597,\n",
      "        -0.0882,  0.0328, -0.0039, -0.0389, -0.0030,  0.0561, -0.0651, -0.0632,\n",
      "         0.0482, -0.0700, -0.0345, -0.0762, -0.0201, -0.0136, -0.0816, -0.0424,\n",
      "        -0.0737,  0.0669,  0.0178,  0.0297,  0.0619,  0.0746,  0.0765,  0.0283,\n",
      "         0.0792,  0.0503,  0.0281, -0.0725, -0.0234,  0.0854, -0.0537, -0.0348,\n",
      "        -0.0500,  0.0113,  0.0683, -0.0191, -0.0604, -0.0312, -0.0199,  0.0021,\n",
      "        -0.0227,  0.0507,  0.0883,  0.0576, -0.0670,  0.0634,  0.0631, -0.0881,\n",
      "         0.0620,  0.0443,  0.0567, -0.0283,  0.0044, -0.0840, -0.0324,  0.0575,\n",
      "        -0.0546,  0.0636,  0.0072,  0.0065, -0.0513, -0.0643, -0.0053, -0.0739,\n",
      "        -0.0710,  0.0527, -0.0597,  0.0858, -0.0564, -0.0338, -0.0188, -0.0251,\n",
      "        -0.0285, -0.0644,  0.0153, -0.0362, -0.0640, -0.0552,  0.0538,  0.0737,\n",
      "        -0.0162,  0.0142,  0.0731, -0.0502,  0.0223, -0.0210,  0.0119, -0.0579,\n",
      "         0.0705,  0.0680,  0.0682, -0.0831, -0.0077,  0.0111,  0.0150, -0.0662,\n",
      "         0.0489,  0.0017, -0.0306,  0.0427, -0.0323, -0.0833, -0.0391, -0.0565,\n",
      "         0.0454,  0.0252,  0.0152,  0.0447, -0.0559, -0.0442,  0.0748,  0.0255,\n",
      "         0.0248,  0.0352, -0.0712,  0.0610,  0.0485, -0.0673,  0.0727,  0.0165,\n",
      "         0.0786,  0.0019, -0.0716,  0.0330,  0.0405,  0.0616, -0.0538,  0.0340,\n",
      "        -0.0491, -0.0646, -0.0356, -0.0680,  0.0416, -0.0598, -0.0497,  0.0777,\n",
      "        -0.0632,  0.0164,  0.0027, -0.0447,  0.0744,  0.0660, -0.0498, -0.0035,\n",
      "         0.0692,  0.0781, -0.0505,  0.0256, -0.0649,  0.0813, -0.0268,  0.0711,\n",
      "         0.0160,  0.0474,  0.0298, -0.0794,  0.0202,  0.0015, -0.0722, -0.0249,\n",
      "         0.0025, -0.0261, -0.0645,  0.0810, -0.0705, -0.0132, -0.0266,  0.0314,\n",
      "        -0.0750,  0.0193,  0.0434, -0.0579,  0.0106,  0.0204, -0.0181, -0.0059,\n",
      "        -0.0073, -0.0484, -0.0213, -0.0161,  0.0697, -0.0479, -0.0180, -0.0347,\n",
      "        -0.0558,  0.0689, -0.0320,  0.0051, -0.0033, -0.0461,  0.0428, -0.0650,\n",
      "        -0.0810, -0.0513, -0.0673,  0.0354, -0.0109, -0.0825,  0.0631, -0.0303,\n",
      "        -0.0141,  0.0485, -0.0774,  0.0023, -0.0234,  0.0219, -0.0700, -0.0045,\n",
      "         0.0438, -0.0123,  0.0723, -0.0387,  0.0097,  0.0871, -0.0776,  0.0645,\n",
      "         0.0159,  0.0287,  0.0727,  0.0296,  0.0854,  0.0813, -0.0072,  0.0789,\n",
      "        -0.0178, -0.0815, -0.0653, -0.0094, -0.0505, -0.0253,  0.0473, -0.0039,\n",
      "         0.0237, -0.0133,  0.0355,  0.0815,  0.0178,  0.0043, -0.0787,  0.0097,\n",
      "        -0.0613, -0.0862,  0.0046, -0.0097, -0.0706,  0.0222,  0.0659,  0.0297],\n",
      "       requires_grad=True) torch.Size([256])\n",
      "out.weight Parameter containing:\n",
      "tensor([[ 0.0560, -0.0049, -0.0444,  ..., -0.0273, -0.0336,  0.0569],\n",
      "        [-0.0544,  0.0020,  0.0238,  ...,  0.0452,  0.0303,  0.0360],\n",
      "        [-0.0036, -0.0236,  0.0552,  ..., -0.0494, -0.0510,  0.0338],\n",
      "        ...,\n",
      "        [ 0.0423,  0.0053,  0.0525,  ...,  0.0086, -0.0568, -0.0031],\n",
      "        [-0.0542,  0.0600,  0.0532,  ...,  0.0159,  0.0037, -0.0350],\n",
      "        [-0.0482, -0.0201,  0.0536,  ..., -0.0204,  0.0420,  0.0432]],\n",
      "       requires_grad=True) torch.Size([10, 256])\n",
      "out.bias Parameter containing:\n",
      "tensor([ 0.0492,  0.0558,  0.0428, -0.0153, -0.0250, -0.0242,  0.0112,  0.0098,\n",
      "        -0.0027, -0.0051], requires_grad=True) torch.Size([10])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter, parameter.size())\n",
    "    \n",
    "# 默认情况下权重会被初始化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用TensorDataset和DataLoader来简化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 数据集构建辅助工具, 将数据集转化为dataset类型.\n",
    "train_ds = TensorDataset(x_train,y_train)\n",
    "# 将dataset类型的数据根据批次产出. 减少计算压力, 想象一下矩阵运算, 庞大的矩阵运算很慢, 用比较小的矩阵运算可以更好的利用内存.\n",
    "train_dl = DataLoader(train_ds,batch_size=bs,shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid,y_valid)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=bs * 2,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# 定义一个数据提取器, 每次提取批次的数据\n",
    "def get_data(train_ds,valid_ds,bs):\n",
    "    return (DataLoader(train_ds,batch_size=bs,shuffle=True),DataLoader(valid_ds,batch_size=bs*2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 一般在训练模型时加上model.train()，这样会正常使用Batch Normalization和 Dropout\n",
    "- 测试的时候一般选择model.eval()，这样就不会使用Batch Normalization和 Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb) # 计算损失, 得到预测结果和应当的结果\n",
    "    if opt is not None:\n",
    "        loss.backward() # 反向传播, 这里反向传播了模型的参数\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# steps 迭代的次数\n",
    "# model 训练的模型\n",
    "# loss_func 损失函数\n",
    "# opt 优化器optimizer\n",
    "# dl 数据迭代器\n",
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for step in range(steps):\n",
    "        model.train() # 训练一次, 下面的for循环才是计算损失函数然后迭代, 这里只是提前训练一次模型\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt) # 调用loss_batch来就算损失函数\n",
    "\n",
    "        # 计算一次损失.\n",
    "        model.eval() \n",
    "        # torch.no_grad() 是一个上下文管理器，用来禁止梯度的计算，通常用来网络推断中，它可以减少计算内存的使用量\n",
    "        # 在这里就是阻止了反向传播的计算. 主要是这里是要求打印损失值,而不需要真正的反向传播计算\n",
    "        with torch.no_grad(): \n",
    "            # 对每一个x,y进行损失值计算然后封装到losses中. 由于在no_grad中所以没有反向传播\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        # 计算当前batch的损失值, 然后输出\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print('当前step:'+str(step), '验证集损失：'+str(val_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "    # 得到模型同时获得优化器, 使用模型中的默认参数\n",
    "    returnModal = Minist_NN()\n",
    "    return returnModal, optim.SGD(returnModal.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%   \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "当前step:0 验证集损失：2.278320760345459\n",
      "当前step:1 验证集损失：2.2438408145904543\n",
      "当前step:2 验证集损失：2.1903117893218993\n",
      "当前step:3 验证集损失：2.102310284423828\n",
      "当前step:4 验证集损失：1.959914992904663\n",
      "当前step:5 验证集损失：1.748250100517273\n",
      "当前step:6 验证集损失：1.4883616399765014\n",
      "当前step:7 验证集损失：1.2371993240356445\n",
      "当前step:8 验证集损失：1.0349678510665894\n",
      "当前step:9 验证集损失：0.8828164986610413\n",
      "当前step:10 验证集损失：0.7705935660362244\n",
      "当前step:11 验证集损失：0.6864762427330017\n",
      "当前step:12 验证集损失：0.6230419251441955\n",
      "当前step:13 验证集损失：0.5739328969478608\n",
      "当前step:14 验证集损失：0.5355695337295532\n",
      "当前step:15 验证集损失：0.5047394303321838\n",
      "当前step:16 验证集损失：0.47971986780166626\n",
      "当前step:17 验证集损失：0.4583516550540924\n",
      "当前step:18 验证集损失：0.4408131406545639\n",
      "当前step:19 验证集损失：0.4259066216468811\n",
      "当前step:20 验证集损失：0.41275083196163176\n",
      "当前step:21 验证集损失：0.401344664812088\n",
      "当前step:22 验证集损失：0.39153350558280947\n",
      "当前step:23 验证集损失：0.3828226818084717\n",
      "当前step:24 验证集损失：0.37505417428016663\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 执行的时候直接调用这个工具类就可以进行训练了\n",
    "train_dl, train_ds = get_data(train_ds,valid_ds,bs)\n",
    "model,opt=get_model()\n",
    "fit(25,model,loss_function,opt,train_dl,valid_dl)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}