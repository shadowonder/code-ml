{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 26, 26])\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 3, 14, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# 简单得卷积\n",
    "x = torch.rand(1, 1, 28, 28)\n",
    "# 第一个参数是输入1个channel, 因为是黑白的\n",
    "# 第二个参数是多少个kernel, 也就是3个kernel,输出3个feature\n",
    "# 3x3 然后步长1, padding是0\n",
    "layer = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=0)\n",
    "output = layer.forward(x)\n",
    "print(output.shape)\n",
    "\n",
    "layer = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)\n",
    "output = layer.forward(x)\n",
    "print(output.shape)\n",
    "\n",
    "layer = nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1)\n",
    "output = layer.forward(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0687, -0.2829, -0.1221],\n",
      "          [ 0.3210, -0.1693,  0.1862],\n",
      "          [-0.0617, -0.1443,  0.1890]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1810, -0.0718,  0.2155],\n",
      "          [ 0.0242, -0.1810, -0.1930],\n",
      "          [ 0.0338, -0.1208, -0.1184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1495, -0.2583,  0.1091],\n",
      "          [-0.0657, -0.0613,  0.1949],\n",
      "          [-0.2283,  0.0678,  0.1106]]]], requires_grad=True)\n",
      "torch.Size([3, 1, 3, 3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 注意, 卷积核是可以被训练的, 它同时会带有一个bias也就是偏执参数b (常量)\n",
    "print(layer.weight)  # 打印kernel, 这里3个channel, 1个input, 3x3的kernel, 因此是[3,1,3,3]\n",
    "print(layer.weight.shape)\n",
    "print(layer.bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 26, 26])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用函数式接口进行卷积\n",
    "x = torch.randn(1, 3, 28, 28)  # 注意输入输出的channel数必须相同\n",
    "w = torch.rand(16, 3, 5, 5)\n",
    "b = torch.rand(16)\n",
    "out = F.conv2d(x, w, b, stride=1, padding=1)\n",
    "print(out.shape)\n",
    "\n",
    "out = F.conv2d(x, w, b, stride=1, padding=2)\n",
    "print(out.shape)\n",
    "\n",
    "out = F.conv2d(x, w, b, stride=2, padding=2)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "上下采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 7, 7])\n",
      "torch.Size([1, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# 下采样, maxpooling\n",
    "x = out\n",
    "\n",
    "layer = nn.MaxPool2d(2, stride=2)\n",
    "out = layer(x)\n",
    "print(out.shape)\n",
    "\n",
    "# 也可以用 nn.AvgPool2d\n",
    "out = F.avg_pool2d(x, 2, stride=2)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 14, 14])\n",
      "torch.Size([1, 16, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "# 上采样, 放大2倍, 将原始数据直接放大. 最后两个维度翻倍\n",
    "x = out\n",
    "out = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "print(out.shape)\n",
    "\n",
    "out = F.interpolate(x, scale_factor=3, mode=\"nearest\")\n",
    "print(out.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 7, 7])\n",
      "torch.Size([1, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.ReLU(inplace=True)\n",
    "out = layer(x)\n",
    "print(out.shape)\n",
    "out = F.relu(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "当数据进行梯度求解的时候用的. 权重进入sigmoid参数的时候很可能是杂乱无章的, 因此很可能出现梯度离散的情况. 所以可以使用normalize进行过变换. 换为一个$N(0,\\sigma)^2$的范围.\n",
    "\n",
    "如果输入数据的范围差距可能很大, 比如$x_1$的范围在0.1-0.2之间,$x_2$的范围在1-255之间, 因此权重的梯度形状就非常的杂乱无章, 因此我们可以进行normalize, 让其输入都控制在一个范围内. 然后梯度的求解尽量快并且不会出现杂乱的问题. \n",
    "\n",
    "![20](./assets/20.png)\n",
    "\n",
    "实现比较简单, 我们对每一个通道进行normalize\n",
    "\n",
    "```python\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现就是batch normalization, 也就是根据batch进行normalize\n",
    "\n",
    "比如我现在有一个batch, batch中存在6个图片3种颜色[6,3,784]. 我们对其进行normalization之后, 我们会得到3个均值和3个方差.\n",
    "\n",
    "![20](./assets/20.png)\n",
    "\n",
    "然后我们就可以将其缩放到$N(0,1)$的正态分布, 然后我们在对这个正态分布增加一个偏执参数$\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16, 784])\n",
      "tensor([0.0499, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0501, 0.0500, 0.0500,\n",
      "        0.0502, 0.0500, 0.0499, 0.0500, 0.0500, 0.0499, 0.0501])\n",
      "tensor([0.9083, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083, 0.9084, 0.9083, 0.9083,\n",
      "        0.9083, 0.9083, 0.9083, 0.9083, 0.9083, 0.9083, 0.9083])\n"
     ]
    }
   ],
   "source": [
    "# 线性数据进行操作\n",
    "x = torch.rand(100, 16, 784)\n",
    "layer = nn.BatchNorm1d(16)\n",
    "out = layer(x)\n",
    "print(out.shape)\n",
    "\n",
    "print(layer.running_mean)\n",
    "print(layer.running_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 7, 7])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "tensor([0.0533, 0.0475, 0.0476, 0.0502, 0.0506, 0.0537, 0.0464, 0.0548, 0.0479,\n",
      "        0.0496, 0.0481, 0.0470, 0.0521, 0.0531, 0.0545, 0.0497])\n",
      "tensor([0.9103, 0.9075, 0.9084, 0.9066, 0.9079, 0.9095, 0.9083, 0.9087, 0.9080,\n",
      "        0.9098, 0.9095, 0.9098, 0.9094, 0.9076, 0.9102, 0.9087])\n"
     ]
    }
   ],
   "source": [
    "# 2d数据进行操作\n",
    "x = torch.rand(1, 16, 7, 7)\n",
    "layer = nn.BatchNorm2d(16)\n",
    "out = layer(x)\n",
    "print(out.shape)\n",
    "\n",
    "# 对于2d来说是存在可导的系数,和编制参数\n",
    "print(layer.weight)  # 对应的就是gamma, 也就是权重缩放的参数\n",
    "print(layer.bias)  # 对应的就是beta偏执参数\n",
    "\n",
    "# 全局数据集\n",
    "print(layer.running_mean)\n",
    "print(layer.running_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([1, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# 在test的时候需要执行eval, 这样才能使用全局的均值和方差\n",
    "print(layer.eval())\n",
    "out = layer(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet\n",
    "\n",
    "定义残差网络一般使用的是残差网络的block, 我们将整个block进行训练, 或者进行bypass. \n",
    "\n",
    "在定义一层以后, 我们定义一个`extra`层, 这个`extra`层就是输入层, 正常的卷积层和`extra`的结果相加. 然后在反向传播的时候, 我们同时训练这个`extra`层. 如果这一层表现得好, 我们的输出层kernel的属性就会非常小, 体现的训练层就会非常清晰, 反之, 我们的`extra`层的kernel就会非常大, 就会跳过这一层的重要性. 大概是这样.\n",
    "\n",
    "`extra`存在的本质就是input和output的shape调整到一样的大小, 使用1x1的kernel进行卷积处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差网络的基本单元\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out) -> None:\n",
    "        \"\"\"\n",
    "        shape必须一致, 一般我们的channel不一致, 一般使用padding和相同kernel大小的话, size是一致的.\n",
    "        输入和输出不一定相同, 需要注意的是我们不太关心输入的大小,因为使用卷积核可以得到我们所需要的featuremap大小\n",
    "        假设我们在这里使用了ch_in = 64, ch_out = 256\n",
    "        \"\"\"\n",
    "        # 输出ch_out个kernel产生的featuremap, 也就是[256,H,W]\n",
    "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
    "        # 输出ch_out个kernel产生的featuremap, 也就是[256,H,W]\n",
    "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
    "        # 到此处我们得到了一个ch_out也就是[256,H,W]的图片\n",
    "\n",
    "        self.extra = nn.Sequential()\n",
    "        if ch_out != ch_in:\n",
    "            # [b, ch_in, h, w] => [b, ch_out, h, w]\n",
    "            self.extra = nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm2d(ch_out),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        我们在前向传播中定义了relu层, 而不是在网络中定义的\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # 对应位置数值进行相加, 就是shortcut和卷积结果相加\n",
    "        out = self.extra(x) + out\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Model 类\n",
    "\n",
    "所有的层需要继承这个类来进行前向传播. 他提供了现成的模块\n",
    "\n",
    "- Linear\n",
    "- Relu\n",
    "- sigmoid\n",
    "- conv2d\n",
    "- convtranspose2d\n",
    "- DropOut\n",
    "\n",
    "容器: Sequential. 可以将所有的层来进行容器堆叠.\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.Conv2d(1,32,5,1,1)\n",
    "    nn.MaxPool2d(2,2)\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNormal2d(32),\n",
    "\n",
    "    nn.Conv2d(32,64,3,1,1)\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNormal2d(64),\n",
    "\n",
    "    nn.Conv2d(64,64,3,1,1)\n",
    "    nn.MaxPool2d(2,2)\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNormal2d(64),\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "![22](./assets/22.png)\n",
    "\n",
    "同时Sequencial可以进行各种意义上的嵌套, 只要是一层一层的结构就可以被嵌套. 因此可以定义自己想要的类. 如果需要的话就可以查看children来观察所有的children. 可以直接将整个网络结构搬入gpu中\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda')\n",
    "net = Net()\n",
    "net.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super(MyLinear, self).__init__()\n",
    "\n",
    "        # requires_grad = True\n",
    "        self.w = nn.Parameter(torch.randn(outp, inp))\n",
    "        self.b = nn.Parameter(torch.randn(outp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.w.t() + self.b\n",
    "        return x\n",
    "\n",
    "\n",
    "## 应用相当广泛\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            Flatten(),\n",
    "            nn.Linear(1 * 14 * 14, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(BasicNet(), nn.ReLU(), nn.Linear(3, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型结构: \n",
    "\n",
    "```python\n",
    "net.load_state_dict(torch.load('ckpt.mdl'))\n",
    "torch.save(net.state_dict(), 'ckpt.mdl')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据增强\n",
    "\n",
    "略\n",
    "\n",
    "```python\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.RandomVerticalFlip(),\n",
    "                       transforms.RandomRotation(15),\n",
    "                       transforms.RandomRotation([90, 180, 270]),\n",
    "                       transforms.Resize([32, 32]),\n",
    "                       transforms.RandomCrop([28, 28]),\n",
    "                       transforms.ToTensor(),\n",
    "                       # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
