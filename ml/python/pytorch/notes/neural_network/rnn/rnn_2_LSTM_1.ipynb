{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "RNN 存在两个特殊的问题, 梯度爆炸(exploding gradient problem), 梯度离散. \n",
    "\n",
    "我们先来看一下rnn的梯度公式\n",
    "\n",
    "![image](./../../assets/23.png)\n",
    "\n",
    "在这里我们可以看到, 最终计算的$W$是以一个乘机计算出来的, 当我们的训练数据极其庞大的时候, 如果w稍大于1, 那么w最终就会变得无穷大. 这就是梯度爆炸.同时, 如果w略小于1, 那么经过无数次迭代以后, 梯度就会变得无穷小. 这也就成为梯度离散.\n",
    "\n",
    "\n",
    "解决梯度爆炸: 如图所示, 这种情况很可能出现梯度爆炸, 比如我们尝试向左进行梯度更新, 但是有个断崖使得error忽然增大. 导致error增大到一个非常大的值, 此时需要设定一个机制. \n",
    "\n",
    "![image](./../../assets/24.jpeg)\n",
    "\n",
    "\n",
    "我们可以设定这个机制, 我们知道W.grad保存了我们的grid值, 我们可以检查一下, 如果大于一个值, 那么就直接设定一个阈值. 我们用梯度的tensor除以梯度的模. 然后在这之上添加一个阈值, 比如15. 这样梯度的方向还是保持不变. 这就是我们这一步走的小一点. *注意我是对梯度grid进行取模, 不是weight*\n",
    "\n",
    "![image](./../../assets/25.png)\n",
    "\n",
    "```python\n",
    "loss = criterion(output, y)\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "# 然后我们对grid进行clip, 我们把tensor传入, 我们将grid clip到一个小于10的范围\n",
    "for p in model.parameters():\n",
    "    print(p.grad.norm())  # 这里我们查看一下梯度, 我们可以尝试简单的搜索\n",
    "    torch.nn.utils.clip_grad_norm_(p, 10)  # 我们可以设置一个if, 如果发现爆炸的情况直接使用这段代码, 对于每一个p进行clip\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度离散也可能出现在cnn中. 在反向传播中, 我们的k进行反向传播数据的时候会累积误差.\n",
    "\n",
    "这个误差会随着网络的结构更加复杂而导致传播误差越来越大. 层数越多, 反向传播到最初一层的误差就会越大. 当我们训练的时候, grid在后面几层反向计算的时候比较大, 但是到了前面几层, grid的计算就非常非常的小. 所以即使网络堆叠的再多, 也未必会完整的得到训练. 这个问题到今天依旧存在于cnn中.\n",
    "\n",
    "对于rnn, 这种问题, 依旧存在于梯度离散问题, 解决这个问题就是LSTM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "long short-term memory networks. 我们的memory节点在NLP设计之初是用来保存语境的, 这样到了后面的节点我们依旧可以懂得我们之前说的是什么意思, 经过长时间的计算, 我们的memory节点可能只能记住最后几个词. LSTM还解决了长度的问题.\n",
    "\n",
    "![image](./../../assets/26.png)\n",
    "\n",
    "我们构建3个门, 我们有选择的让输入, 输出, h和h的梯度计算进行在当前节点上. 这里的门就是sigmoid函数, 我们就可以启动控制输出的方式. 图中, $\\sigma$就是sigmoid函数.\n",
    "\n",
    "需要注意, 在定义网络的时候, 我们定义h的大小同时反映了C的大小, 因为两个矩阵需要做运算, 因此大小是相同的.\n",
    "\n",
    "$out, (ht,ct) = lstm(x,[ht_1,ct_1])$\n",
    "\n",
    "out也是所有层所有sequence的所有输出, 注意这里只有h不需要c, ct和ht都是结果\n",
    "- x: [seq,b,vec]\n",
    "- h/c: [layers,b,h]\n",
    "- out: [seq,b,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(100, 20, num_layers=4)\n",
      "torch.Size([10, 3, 20]) torch.Size([4, 3, 20]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(100, 20, 4)  # 20个hidden, 100维向量\n",
    "print(lstm)\n",
    "\n",
    "x = torch.randn(10, 3, 100)\n",
    "out, (h, c) = lstm(x)  # lstm(x,[h,c]) 如果不指定就直接使用0\n",
    "print(out.shape, h.shape, c.shape)\n",
    "# H: [4,3,20] 一个4层, 每一层3个单词, 每个单词20个向量用来记忆.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种方式, 使用cell\n",
    "\n",
    "相比于直接定义lstm,这种更灵活. 在定义的时候都是一样的, 但是前向传播不太一样\n",
    "\n",
    "$ht_1,ct_1 = lstmcell(xt,[ht_0,ct_0])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "## 第二种方式, 使用cell\n",
    "cell = nn.LSTMCell(100, 20)\n",
    "h = torch.zeros(3, 20)\n",
    "c = torch.zeros(3, 20)\n",
    "x = torch.randn(3, 100)\n",
    "for xt in x:\n",
    "    h, c = cell(x, [h, c])\n",
    "print(h.shape, c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 定义多层\n",
    "x = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.LSTMCell(100, 30)\n",
    "cell2 = nn.LSTMCell(30, 20)\n",
    "h1 = torch.zeros(3, 30)\n",
    "c1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 20)\n",
    "c2 = torch.zeros(3, 20)\n",
    "for xt in x:\n",
    "    h1, c1 = cell1(xt, [h1, c1])\n",
    "    h2, c2 = cell2(h1, [h2, c2])\n",
    "\n",
    "print(h2.shape, c2.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f5fd407444611a7de458766408d8f0b30a2f26501ee96cb5dac23e78b64d77c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
