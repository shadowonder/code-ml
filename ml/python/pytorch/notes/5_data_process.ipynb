{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型数据处理\n",
    "\n",
    "## 验证集\n",
    "\n",
    "一般用dataloader, 然后直接用不同的文件夹处理\n",
    "\n",
    "## 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 50000 test: 10000\n",
      "db1: 50000 db2: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "train_db = datasets.MNIST(\n",
    "    '../data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]))\n",
    "train_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_db = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('train:', len(train_db), 'test:', len(test_db))\n",
    "print('db1:', len(train_db), 'db2:', len(val_db))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型过拟合\n",
    "\n",
    "一般处理多种情况\n",
    "\n",
    "- 更多的数据\n",
    "- 减小模型复杂度, regulization正则化\n",
    "- dropout\n",
    "- 数据增强\n",
    "- 提前结束运算\n",
    "\n",
    "添加L2正则:\n",
    "```python\n",
    "device = torch.device('cuda:0')\n",
    "net = MLP().to(device)\n",
    "# weight_decay就是lambda 0.01就是l2\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.01) \n",
    "criteon = nn.CrossEntropyLoss().to(device)\n",
    "```\n",
    "\n",
    "L1正则比较复杂pytorch并没有携带任何L1代码,需要自己对w进行修正\n",
    "```python\n",
    "regularization_loss = 0\n",
    "for param in model.parameters():\n",
    "    regularization_loss += torch.sum(torch.abs(param))\n",
    "classify_loss = criteon(logits, target)\n",
    "loss = classify_loss + 0.01 * regularization_loss\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量 momentum\n",
    "\n",
    "就是在计算梯度的时候添加一个动量让其可以冲过局部最小值. 这里就是增加了一个$\\beta$参数让其可以对原始的参数进行增大. 此时我们就得到了一个新的$\\beta$函数, 然后用这个函数进行计算.\n",
    "\n",
    "$$z^{k+1}=\\beta{z^k}+\\Delta{f(w^k)}$$\n",
    "$$w^{k+1}=w^k-\\alpha{z^{k+1}}$$\n",
    "\n",
    "pytorch中momentum只需要添加一个参数就可以\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.01, momentum=args.momentum) \n",
    "```\n",
    "\n",
    "## 梯度递减\n",
    "\n",
    "第一种方法: 使用ReduceLROnPlateau\n",
    "\n",
    "这里就是我们的目标是min也就是减小, 我们的目标是监听loss, 如果我们等待了paience次(也就是默认10次)loss没有减小, name我们就减少lr. 减少factor的比重也就是0.1.\n",
    "\n",
    "```python\n",
    "# ReduceLROnPlateau(optimier,mode='min',factor=0.1,paience=10,verbose=False,threadhold=0.0001,threshold_mode='rel', cooldown=0,min_lr=0,eps=1e-8)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "for epoch in xrange(start,arg.epochs):\n",
    "    #...\n",
    "    scheduler.step(loss)\n",
    "```\n",
    "\n",
    "第二种方法使用暴力衰减. 直接定义多少次减少\n",
    "```python\n",
    "scheduler = StepLR(optimizer, step_size=30,gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    #...\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stop\n",
    "\n",
    "略\n",
    "\n",
    "## dropout\n",
    "\n",
    "需要注意的是pytorch是drop 概率, 而tensorflow是keep概率, 两个是完全相反的. 巨坑\n",
    "\n",
    "同时在test的时候一定要eval否则就会随机drop\n",
    "\n",
    "```python\n",
    "self.model = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.Dropout(0.5), # 随机减少50%个神经元链接\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.Dropout(0.5), # 随机减少50%个神经元链接\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    ")\n",
    "```\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "根据方差进行分布选择. 本身的梯度应该是所有点的梯度平均值, 但是不可能, 所以就是每一个批次进行梯度平均值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
