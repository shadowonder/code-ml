{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 细节\n",
    "\n",
    "## 交叉熵\n",
    "\n",
    "在分类任务中我们不能单纯的将mse输入回去. 因为很可能输入回去的错误会使得梯度消失. 比如: 我们有3个正确2个错误, 计算回去的mse是一个值, 然后经过计算以后, 我们的权重w更新了, 但是输出结果还是3个正确2个错误, 因此mse并没有改变. 此时不断地修正w很可能导致梯度消失.\n",
    "\n",
    "- sigmoid + mse 很可能导致梯度小时\n",
    "- 收敛很慢\n",
    "\n",
    "> 需要注意的是并不是一定不要用mse, mse在很多种情况下还是非常的优秀的. 只是他会造成很多问题, 但是它计算起来会很简单, 比如metalearning中使用mse会计算很快.\n",
    "\n",
    "因此为了避免这种情况我们需要使用交叉熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "tensor(76.0477)\n",
      "tensor(76.0477)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 784)\n",
    "w = torch.randn(10, 784)\n",
    "\n",
    "logits = x@w.t()  # 自己计算一次预测\n",
    "print(logits.shape)\n",
    "\n",
    "# 使用softmax计算一次结果\n",
    "pred = F.softmax(logits, dim=1)\n",
    "print(pred.shape)\n",
    "\n",
    "# 使用交叉熵必须使用logits, 不能使用pred因为交叉熵已经继承了softmax\n",
    "print(F.cross_entropy(logits, torch.tensor([3])))\n",
    "\n",
    "# 如果一定要使用交叉熵, 可以使用none negative进行预测. 但是必须进行一次log算法\n",
    "# 得到一个log的predict\n",
    "pred_log = torch.log(pred)\n",
    "print(F.nll_loss(pred_log, torch.tensor([3])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类问题\n",
    "\n",
    "使用交叉熵优化多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 输入层\n",
    "w1, b1 = torch.randn(200, 784, requires_grad=True), torch.zeros(\n",
    "    200, requires_grad=True)\n",
    "w2, b2 = torch.randn(200, 200, requires_grad=True), torch.zeros(\n",
    "    200, requires_grad=True)\n",
    "# 输出层\n",
    "w3, b3 = torch.randn(10, 200, requires_grad=True), torch.zeros(\n",
    "    10, requires_grad=True)\n",
    "\n",
    "# 添加kaiming初始化\n",
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)\n",
    "\n",
    "\n",
    "# 使用relu函数进行前向传播辅助计算\n",
    "def forward(x):\n",
    "    x = x@w1.t() + b1\n",
    "    x = F.relu(x)\n",
    "    x = x@w2.t() + b2\n",
    "    x = F.relu(x)\n",
    "    x = x@w3.t() + b3\n",
    "    x = F.relu(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定一mnist数据读取器\n",
    "batch_size = 200\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.569833\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 1.055235\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.910035\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 7083/10000 (71%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.801024\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.773433\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.749979\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 7235/10000 (72%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.749595\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.635862\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.771702\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 7329/10000 (73%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.865054\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.687739\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.712944\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 7386/10000 (74%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.788834\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.668117\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.641329\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 8267/10000 (83%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.558467\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.464136\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.530739\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 8335/10000 (83%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.393331\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.397093\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.471932\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 8374/10000 (84%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.448791\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.392639\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.337378\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 8388/10000 (84%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.438278\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.484702\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.388345\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8419/10000 (84%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.632718\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.394388\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.406472\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8437/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28)\n",
    "\n",
    "        # forward中不能出现softmax因为后面我们要用entropy来计算, 其中已经包含了softmax了\n",
    "        logits = forward(data)\n",
    "        loss = criteon(logits, target)  # 使用交叉熵计算loss, 而不是mse\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # print(w1.grad.norm(), w2.grad.norm())\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        logits = forward(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如果不使用数据初始化使用的就是高斯初始化, 此时可以发现上面的lose和准确率被锁定在10%. loss一直不变. \n",
    "\n",
    "这里, 我们的网络结构非常的简单, 同时我们使用的是relu函数, relu函数不会出现梯度离散的情况. **loss信息长时间不变表明了梯度信息接近于0**, 因此我们需要明白为什么梯度为0.\n",
    "\n",
    "出现梯度为0的原因一方面是learning rate过大导致gradient vanish, 另一方面就是数据初始化问题.\n",
    "\n",
    "此时我们使用kaiming的数据初始化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0257,  0.0396, -0.0005,  ...,  0.0971, -0.0170,  0.0907],\n",
       "        [-0.1090, -0.1282, -0.0603,  ...,  0.0797,  0.0831,  0.0948],\n",
       "        [-0.0545, -0.0405,  0.0252,  ...,  0.0263, -0.0316, -0.2278],\n",
       "        ...,\n",
       "        [ 0.0206, -0.0957, -0.0516,  ...,  0.0515, -0.0398,  0.0632],\n",
       "        [-0.0486, -0.0016,  0.1319,  ..., -0.0299,  0.1092,  0.0035],\n",
       "        [ 0.1982,  0.0827, -0.0673,  ...,  0.1431, -0.0279, -0.0217]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用kaiming初始化, 初始化所有的权重\n",
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数以及gpu加速\n",
    "\n",
    "除了relu函数意外, 我们可以使用leakyrelu函数. Relu函数不容易出现梯度离散的情况因为输出的结果相对于输入是相通的, 但是也只是\"不容易\"而不是不会. 因此,相比于relu函数, leakyrelu就会更不容易出现离散的情况.\n",
    "\n",
    "```python\n",
    "self.model = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    ")\n",
    "```\n",
    "\n",
    "relu函数其实还是不连续的,因此我们可以使用另一个函数`SELU`, 他和RELU函数相似但是是具有数学意义的, 更加光滑(很少用)\n",
    "\n",
    "![selu](./assets/18.png)\n",
    "\n",
    "softplus也是relu的加强版, 是完全的数学函数(很少用)\n",
    "\n",
    "![0](./assets/19.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu加速\n",
    "\n",
    "`.to(device)`方法返回一个reference.\n",
    "\n",
    "```python\n",
    "# 如果使用mac的话使用mps gpu芯片\n",
    "# device = torch.device(\"mps\") \n",
    "device = torch.device('cuda:0')\n",
    "# 对模块使用.to(device)方法调用和之前的使用方式是一摸一样的, 是一个inplace的操作\n",
    "net = MLP().to(device) \n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss().to(device) # loss放入gpu\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28)\n",
    "        # 如果使用to(device)方法返回的就不是一样的模块\n",
    "        # 这个数据是可以back反向传播的,但是这个操作会产生两个tensor, 一个是cpu上, 一个是gpu上\n",
    "        data, target = data.to(device), target.cuda() # 两种方法, 一般使用第一种方法\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "Generalize Performance 过拟合\n",
    "\n",
    "需要引用测试集.\n",
    "\n",
    "- test on epoch\n",
    "- test on step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor([5, 3, 2, 2])\n",
      "tensor([5, 3, 2, 2])\n",
      "tensor([False,  True,  True, False])\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "logits = torch.rand(4, 10)\n",
    "pred = F.softmax(logits, dim=1)\n",
    "print(pred.shape)\n",
    "pred_label = pred.argmax(dim=1)\n",
    "print(pred_label)\n",
    "\n",
    "print(logits.argmax(dim=1))\n",
    "\n",
    "label = torch.tensor([9, 3, 2, 4])\n",
    "correct = torch.eq(pred_label, label) # 计算相等的元素\n",
    "print(correct)\n",
    "print(correct.sum().float().item()/4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般在epoch中直接进行test\n",
    "\n",
    "```python\n",
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "# 每一个test文件输入到网络文件中\n",
    "for data, target in test_loader:\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    data, target = data.to(device), target.cuda()\n",
    "    logits = net(data)\n",
    "    test_loss += criteon(logits, target).item()\n",
    "\n",
    "    pred = logits.argmax(dim=1)\n",
    "    correct += pred.eq(target).float().sum().item()\n",
    "\n",
    "# 正确数据除以总数据集\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化\n",
    "\n",
    "Visdom可视化. Tensorflow 使用tensorboard来进行展示, pytorch使用tensorboardx来进行展示.\n",
    "\n",
    "```shell\n",
    "pip install tensorboardx\n",
    "```\n",
    "\n",
    "Tensorboard 的本质就是一个服务器, 然后我们将数据写入那个服务器. 但是有一个问题就是writer是作用在cpu上的, 我们需要将数据转换到cpu上, 然后才能通过cpu写入数据. 而且Tensorboard会将数据写入硬盘, 占用资源会非常大. 而且每30秒更新一次.\n",
    "\n",
    "```python\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "writer.add_scaler('data/scalar1', dummy_s1[0], n_iter)\n",
    "\n",
    "writer.add_scalers(\n",
    "    'data/scalar_group',\n",
    "    {'xsinx': n_iter*np.sin(n_iter),\n",
    "     \"xcosx\": n_iter*np.cos(n_iter),\n",
    "     \"arctanx\": np.arctan(n_iter)}, n_iter)\n",
    "\n",
    "writer.add_image('Image',x,n_iter)\n",
    "writer.add_text('Text','text logged at step' + str(n_iter), n_iter)\n",
    "\n",
    "writer.close()\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Visdom的优势在于可以接收原生的tensor. \n",
    "\n",
    "```shell\n",
    "pip install visdom\n",
    "python -m visdom.server # 开启服务器进程\n",
    "```\n",
    "\n",
    "如果不行, 就去下载服务器, 或者直接下载源代码进行编译. 如果下载源代码就进入源代码目录使用`pip install -e .`来安装. \n",
    "\n",
    "也可以考虑docker安装. \n",
    "\n",
    "`docker run -it -p 8097:8097 --name visdom hypnosapos/visdom -d`\n",
    "\n",
    "注意docker直接下载也可能出现芯片错误, 可以自己编译\n",
    "\n",
    "```python\n",
    "from visdom import Visdom\n",
    "viz = Visdom()\n",
    "# 第一个参数是y, 第二个参数是x, win哪一个窗口, 如果不指定就是用默认窗口main\n",
    "viz = line([0.],[0.], win=\"train_loss\", opts=dict(title=\"train lose\"))\n",
    "# 然后一次一次添加数据, update=append表示当前这个是追加动作\n",
    "# 同样, 第一个值是y,第二个值是x, win哪一个窗口\n",
    "# 注意这里也是一个numpy数据\n",
    "viz.line([loss.item()],[global_step],win='train_loss', update='append')\n",
    "```\n",
    "\n",
    "多条线进行数据输入\n",
    "\n",
    "```python\n",
    "from visdom import Visdom\n",
    "viz = Visdom()\n",
    "viz.line([[test_loss, correct / len(test_loader.dataset)]],\n",
    "            [global_step], win='test', update='append')\n",
    "viz.images(data.view(-1, 1, 28, 28), win='x')\n",
    "viz.text(str(pred.detach().cpu().numpy()), win='pred',\n",
    "            opts=dict(title='pred'))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
