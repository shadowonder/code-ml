{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 梯度\n",
    "\n",
    "pytorch可以更好地将反向传播进行求导. 梯度就是函数变化的一个标量, 下图的箭头的长度就是梯度\n",
    "\n",
    "![6](./assets/6.png)\n",
    "\n",
    "$\\theta_{t+1}=\\theta_{t}-\\alpha_t\\Delta{f(\\theta_{t})}$\n",
    "\n",
    "就是对每一个$\\theta$进行偏微分(其他看做常数对这个变量求导的那个)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y=wx+b$, w偏微分: $x$, b偏微分: $1$, 因此函数的梯度在任何位置的梯度就是$(x, 1)$\n",
    "- $y=w^2x+b^2$, w偏微分: $2xw$, b偏微分: $2b$, 因此函数的梯度在任何位置的梯度就是$(2wx, 2b)$\n",
    "- $y=e^wx+e^b$, w偏微分: $e^wx$, b偏微分: $e^b$, 因此函数的梯度在任何位置的梯度就是$(e^wx, e^b)$\n",
    "- $f[y-(xw+b)]$, w偏微分: $2(y-xw-b)x$, b偏微分: $2(y-wx-b)$, 因此函数的梯度在任何位置的梯度就是$(2(y-xw-b)x, 2(y-wx-b))$\n",
    "- $f(ylog(wx+b))$, 要知道$(log_ex)'=\\frac{1}{x}$, w偏微分: $\\frac{y}{wx+b}x$, b偏微分: $\\frac{y}{wx+b}$, 因此函数的梯度在任何位置的梯度就是$(\\frac{y}{wx+b}x, \\frac{y}{wx+b})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要求导可以手动定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4379,  0.3458,  0.2249, -0.8641],\n",
       "        [-1.3174,  0.6861,  0.2538,  1.2211],\n",
       "        [ 0.4777, -1.4014, -0.1084, -1.3358]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4, requires_grad=True)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4379,  0.3458,  0.2249, -0.8641],\n",
       "        [-1.3174,  0.6861,  0.2538,  1.2211],\n",
       "        [ 0.4777, -1.4014, -0.1084, -1.3358]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或者使用pytorch定义\n",
    "x.requires_grad = True\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4723, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(3, 4, requires_grad=True)\n",
    "t = x+b\n",
    "y = t.sum()\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()  # 反向传播\n",
    "b.grad  # 求导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 神经网络的求偏导过程:\n",
    "![image](./assets/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算流程\n",
    "x = torch.randn(1)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "y = w*x\n",
    "z = y+b\n",
    "# y也需要, 因为w是在y的后面计算w必须先计算y\n",
    "x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf, b.is_leaf, w.is_leaf, y.is_leaf, z.is_leaf  # leaf节点, 是不是最后一个计算偏导的节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1368])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward(retain_graph=True)  # 如果不对梯度进行清零的话, 梯度默认会累加起来\n",
    "w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是, 神经网络的训练一定要进行参数初始化, 这个对于找到最优解有至关重要的作用.\n",
    "\n",
    "## Escape Minima\n",
    "\n",
    "很多时候寻找到的是局部最小值, 于是我们可以添加一个动量让其继续向前移动一部分时间.\n",
    "\n",
    "![7](./assets/7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid, 容易出现梯度离散和梯度爆炸\n",
    "a = torch.linspace(-100, 100, 10)\n",
    "torch.sigmoid(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tanh 在rnn中用的比较多\n",
    "a = torch.linspace(-100, 100, 10)\n",
    "torch.tanh(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.linspace(-1, 1, 10)\n",
    "torch.relu(a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "MSE = $\\sum[y-(wx+b)]^2$\n",
    "\n",
    "L2-norm = $||y-(wx+b)||_2$\n",
    "\n",
    "loss = $norm(y-(wx+b))^2$\n",
    "\n",
    "两种方法根据loss对目标参数求导: \n",
    "\n",
    "- `torch.autograd.grad(loss,[w1,w2,...])`\n",
    "- `loss.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([2.], requires_grad=True) tensor(1., grad_fn=<MseLossBackward0>)\n",
      "autograd: (tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "出现错误\n",
    "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "因为 requires_grad 需要配置为 True, pytorch需要在建立的时候建立可导信息\n",
    "\"\"\"\n",
    "\n",
    "# 自动计算梯度\n",
    "x = torch.ones(1)\n",
    "w = torch.full([1], 2.0)  # 需要使用float才可以进行梯度运算\n",
    "\n",
    "w.requires_grad_()\n",
    "\n",
    "# 计算loss\n",
    "# 第一个参数是predict, 第二个参数是[w_1,w_2...]\n",
    "# (1-2)^2 = 1\n",
    "mse = F.mse_loss(torch.ones(1), w*x)\n",
    "print(x, w, mse)\n",
    "\n",
    "print(\"autograd:\", torch.autograd.grad(mse, [w]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上面使用autograd计算梯度的方法意外也可以使用backward()方法来计算梯度\n",
    "\n",
    "backward方法会将路径上所有相关数据全部进行梯度运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MseLossBackward0>)\n",
      "None\n",
      "backward: tensor([2.])\n",
      "backward, w norm: tensor(2., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算梯度\n",
    "x = torch.ones(1)\n",
    "w = torch.full([1], 2.0, requires_grad=True)\n",
    "\n",
    "# 计算loss\n",
    "mse = F.mse_loss(torch.ones(1), w*x)\n",
    "print(mse)\n",
    "\n",
    "\"\"\"\n",
    "注意, grad是储存在每一个需要grad参数中的, 因此backward方法的结果是存储在w.grad参数中, 而不是backward方法的返回值\n",
    "而backward方法在计算的时候是直接将grad累加进入每一个需要计算的参数中, 如果需要重新计算需要清空grad信息\n",
    "\"\"\"\n",
    "print(w.grad)  # None\n",
    "mse.backward()\n",
    "print(\"backward:\", w.grad)  # 2\n",
    "# 打印norm, 一般情况下会直接打印grad信息, 但是如果grad过大或者过小就可以打印norm信息来查看矩阵的norm\n",
    "print(\"backward, w norm:\", w.norm())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "\n",
    "相比于sigmod函数, 我们不一定需要其给我们的最终结果\n",
    "\n",
    "在最终结果中, 我们需要所有的概率和相加等于1, 因此我们可以使用softmax\n",
    "\n",
    "![8](./assets/8.png)\n",
    "\n",
    "softmax可以把更大的放的更大, 更小的放的更小. 类似金字塔效应."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9376, 0.5373, 0.6736], requires_grad=True) None\n",
      "tensor([0.4102, 0.2748, 0.3150], grad_fn=<SoftmaxBackward0>)\n",
      "A grad tensor([ 0.0551, -0.0375, -0.0176])\n",
      "(tensor([-0.1127,  0.1993, -0.0866]),)\n",
      "(tensor([-0.1292, -0.0866,  0.2158]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3)  # 生成一个长度为3的随机float list\n",
    "a.requires_grad_()\n",
    "print(a, a.grad)\n",
    "\n",
    "p = F.softmax(a, dim=0)  # 生成概率, 此时需要告诉softmax对哪一个维度进行softmax.\n",
    "print(p)\n",
    "\n",
    "\"\"\"\n",
    "当输出不是标量时，调用.backward()就会出错\n",
    "显示声明输出的类型作为参数传入,且参数的大小必须要和输出值的大小相同, 因此需要指定 p.data为输入数据\n",
    "\"\"\"\n",
    "p.backward(p, retain_graph=True)\n",
    "# 第二次使用backward的时候汇报错误:\n",
    "# RuntimeError: Trying to backward through the graph a second time (or directly access saved\n",
    "# tensors after they have already been freed). Saved intermediate values of the graph are freed when you call\n",
    "# .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the\n",
    "# graph a second time or if you need to access saved tensors after calling backward.\n",
    "# 意思就是需要指定 retain-graph=True 不去销毁计算梯度的图\n",
    "p.backward(p)\n",
    "print(\"A grad\", a.grad)\n",
    "\n",
    "# 重新建图\n",
    "# 注意: backward函数的loss必须是一个dim=1长度为1的数, 不能有多个量. 损失必须是一个单一的定值, 不能是一个向量.\n",
    "# 否则的话就是代码或者逻辑有问题\n",
    "p = F.softmax(a, dim=0)\n",
    "# 由于长度不为1, 所以对p的多个变量进行返回\n",
    "print(torch.autograd.grad(p[1], [a], retain_graph=True))\n",
    "print(torch.autograd.grad(p[2], [a]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
